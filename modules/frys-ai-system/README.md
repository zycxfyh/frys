# Frys AI System (frys-ai-system)

Frys AI System æ˜¯ç³»ç»Ÿçš„AIæ¨ç†å¼•æ“ï¼Œæä¾›äº†å¤šæ¨¡æ€AIèƒ½åŠ›ã€æ¨¡å‹ç®¡ç†å’Œæ™ºèƒ½ç¼“å­˜ã€‚å®ƒé›†æˆäº†Sira AI Gatewayï¼Œæ”¯æŒå¤šç§AIæ¨¡å‹å’Œæ¨ç†ä»»åŠ¡ã€‚

## ğŸ¯ è®¾è®¡ç†å¿µ

**å¤šæ¨¡æ€AIæ¨ç†å¹³å°ï¼Œä¸ºä¸šåŠ¡ç³»ç»Ÿæä¾›å¼ºå¤§çš„AIèƒ½åŠ›**

### æ ¸å¿ƒç‰¹æ€§
- **ğŸ§  å¤šæ¨¡æ€æ¨ç†**: æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€
- **ğŸ”„ åŠ¨æ€æ¨¡å‹åŠ è½½**: æŒ‰éœ€åŠ è½½å’Œå¸è½½AIæ¨¡å‹
- **ğŸ’¾ æ™ºèƒ½ç¼“å­˜**: æ¨ç†ç»“æœç¼“å­˜å’Œæ¨¡å‹é¢„çƒ­
- **âš–ï¸ è´Ÿè½½å‡è¡¡**: å¤šæ¨¡å‹å®ä¾‹é—´çš„æ™ºèƒ½è´Ÿè½½å‡è¡¡
- **ğŸ“Š æ€§èƒ½ç›‘æ§**: è¯¦ç»†çš„æ¨ç†æ€§èƒ½æŒ‡æ ‡å’Œåˆ†æ
- **ğŸ”’ å®‰å…¨éš”ç¦»**: æ¨¡å‹æ¨ç†çš„å®‰å…¨æ²™ç®±ç¯å¢ƒ

### æ¶æ„ä¼˜åŠ¿
- **é«˜æ€§èƒ½**: GPUåŠ é€Ÿå’Œæ¨¡å‹ä¼˜åŒ–
- **å¯æ‰©å±•**: æ”¯æŒå¤§è§„æ¨¡å¹¶å‘æ¨ç†
- **çµæ´»æ€§**: æ’ä»¶åŒ–æ¨¡å‹å’Œæ¨ç†å¼•æ“
- **å¯é æ€§**: è‡ªåŠ¨æ•…éšœè½¬ç§»å’Œæ¢å¤
- **ç»æµæ€§**: æŒ‰éœ€èµ„æºåˆ†é…å’Œæˆæœ¬ä¼˜åŒ–

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
frys-ai-system/
â”œâ”€â”€ Core Engine           # ğŸ§  æ ¸å¿ƒæ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ Model Manager        # æ¨¡å‹ç®¡ç†å™¨
â”‚   â”œâ”€â”€ Inference Runtime    # æ¨ç†è¿è¡Œæ—¶
â”‚   â”œâ”€â”€ Resource Scheduler   # èµ„æºè°ƒåº¦å™¨
â”‚   â””â”€â”€ Cache Manager        # ç¼“å­˜ç®¡ç†å™¨
â”œâ”€â”€ Multi-Modal Support  # ğŸ­ å¤šæ¨¡æ€æ”¯æŒ
â”‚   â”œâ”€â”€ Text Processing     # æ–‡æœ¬å¤„ç†
â”‚   â”œâ”€â”€ Vision Processing   # è§†è§‰å¤„ç†
â”‚   â”œâ”€â”€ Audio Processing    # éŸ³é¢‘å¤„ç†
â”‚   â””â”€â”€ Cross-Modal Fusion  # è·¨æ¨¡æ€èåˆ
â”œâ”€â”€ Sira Integration    # ğŸ”— Sira AIç½‘å…³é›†æˆ
â”‚   â”œâ”€â”€ Gateway Client      # ç½‘å…³å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ Load Balancer       # è´Ÿè½½å‡è¡¡å™¨
â”‚   â”œâ”€â”€ Failover Manager    # æ•…éšœè½¬ç§»ç®¡ç†
â”‚   â””â”€â”€ Performance Monitor # æ€§èƒ½ç›‘æ§å™¨
â”œâ”€â”€ Intelligent Caching # ğŸ’¡ æ™ºèƒ½ç¼“å­˜
â”‚   â”œâ”€â”€ Result Cache        # ç»“æœç¼“å­˜
â”‚   â”œâ”€â”€ Model Cache         # æ¨¡å‹ç¼“å­˜
â”‚   â”œâ”€â”€ Embedding Cache     # åµŒå…¥ç¼“å­˜
â”‚   â””â”€â”€ Cache Strategy      # ç¼“å­˜ç­–ç•¥
â””â”€â”€ Plugin Ecosystem    # ğŸ”Œ æ’ä»¶ç”Ÿæ€
    â”œâ”€â”€ Model Plugins       # æ¨¡å‹æ’ä»¶
    â”œâ”€â”€ Processor Plugins   # å¤„ç†æ’ä»¶
    â”œâ”€â”€ Optimizer Plugins   # ä¼˜åŒ–æ’ä»¶
    â””â”€â”€ Integration Plugins # é›†æˆæ’ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```rust
use frys_ai_system::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆ›å»ºAIç³»ç»Ÿé…ç½®
    let config = AISystemConfig {
        max_concurrent_requests: 100,
        model_cache_size: 10,
        result_cache_ttl: Duration::from_secs(3600),
        enable_sira_integration: true,
        sira_gateway_url: "http://sira-gateway:8080".to_string(),
        gpu_acceleration: true,
        monitoring_enabled: true,
    };

    // åˆå§‹åŒ–AIç³»ç»Ÿ
    let ai_system = AISystem::new(config).await?;
    println!("Frys AI System initialized successfully!");

    // æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
    let text_request = AIRequest {
        model: "gpt-4".to_string(),
        task: AITask::TextGeneration,
        input: serde_json::json!({
            "prompt": "Write a short story about AI",
            "max_tokens": 500,
            "temperature": 0.7
        }),
        options: AIRequestOptions {
            priority: Priority::Normal,
            timeout: Some(Duration::from_secs(60)),
            caching_enabled: true,
        },
    };

    let text_response = ai_system.infer(text_request).await?;
    println!("Generated text: {}", text_response.output);

    // å›¾åƒåˆ†ç±»ä»»åŠ¡
    let image_request = AIRequest {
        model: "resnet50".to_string(),
        task: AITask::ImageClassification,
        input: serde_json::json!({
            "image_url": "https://example.com/image.jpg",
            "top_k": 5
        }),
        options: AIRequestOptions {
            priority: Priority::High,
            timeout: Some(Duration::from_secs(30)),
            caching_enabled: true,
        },
    };

    let image_response = ai_system.infer(image_request).await?;
    println!("Classification results: {:?}", image_response.output);

    Ok(())
}
```

### æµå¼æ¨ç†

```rust
// æµå¼æ–‡æœ¬ç”Ÿæˆ
let stream_request = AIRequest {
    model: "gpt-4".to_string(),
    task: AITask::TextGeneration,
    input: serde_json::json!({
        "prompt": "Write a poem about technology",
        "max_tokens": 200,
        "stream": true
    }),
    options: Default::default(),
};

let mut stream = ai_system.infer_stream(stream_request).await?;

while let Some(chunk) = stream.next().await {
    match chunk {
        StreamChunk::Text { text, .. } => {
            print!("{}", text);
            io::stdout().flush().await?;
        }
        StreamChunk::Done => break,
        StreamChunk::Error { message } => {
            eprintln!("Stream error: {}", message);
            break;
        }
    }
}
```

### æ‰¹é‡æ¨ç†

```rust
// æ‰¹é‡å¤„ç†å¤šä¸ªæ¨ç†è¯·æ±‚
let batch_requests = vec![
    AIRequest {
        model: "bert-base".to_string(),
        task: AITask::TextEmbedding,
        input: serde_json::json!({"text": "First document"}),
        options: Default::default(),
    },
    AIRequest {
        model: "bert-base".to_string(),
        task: AITask::TextEmbedding,
        input: serde_json::json!({"text": "Second document"}),
        options: Default::default(),
    },
    // ... æ›´å¤šè¯·æ±‚
];

let batch_results = ai_system.infer_batch(batch_requests).await?;
for (i, result) in batch_results.into_iter().enumerate() {
    println!("Batch result {}: {:?}", i, result.output);
}
```

## ğŸ¤– å¤šæ¨¡æ€AIèƒ½åŠ›

### æ–‡æœ¬å¤„ç†

```rust
// æ–‡æœ¬åˆ†ç±»
let classification_request = AIRequest {
    model: "bert-classifier".to_string(),
    task: AITask::TextClassification,
    input: serde_json::json!({
        "text": "I love this product! It's amazing.",
        "labels": ["positive", "negative", "neutral"]
    }),
    options: Default::default(),
};

let result = ai_system.infer(classification_request).await?;
println!("Classification: {:?}", result.output);

// å‘½åå®ä½“è¯†åˆ«
let ner_request = AIRequest {
    model: "spacy-ner".to_string(),
    task: AITask::NamedEntityRecognition,
    input: serde_json::json!({
        "text": "Apple Inc. was founded by Steve Jobs in Cupertino."
    }),
    options: Default::default(),
};

let entities = ai_system.infer(ner_request).await?;
println!("Entities: {:?}", entities.output);
```

### è§†è§‰å¤„ç†

```rust
// å›¾åƒåˆ†ç±»
let image_classification = AIRequest {
    model: "efficientnet-b0".to_string(),
    task: AITask::ImageClassification,
    input: serde_json::json!({
        "image": base64_image_data,
        "top_k": 3
    }),
    options: Default::default(),
};

let predictions = ai_system.infer(image_classification).await?;
println!("Top predictions: {:?}", predictions.output);

// ç›®æ ‡æ£€æµ‹
let object_detection = AIRequest {
    model: "yolov5".to_string(),
    task: AITask::ObjectDetection,
    input: serde_json::json!({
        "image": image_url,
        "confidence_threshold": 0.5
    }),
    options: Default::default(),
};

let detections = ai_system.infer(object_detection).await?;
println!("Detected objects: {:?}", detections.output);
```

### éŸ³é¢‘å¤„ç†

```rust
// è¯­éŸ³è¯†åˆ«
let speech_recognition = AIRequest {
    model: "whisper-base".to_string(),
    task: AITask::SpeechRecognition,
    input: serde_json::json!({
        "audio": audio_base64_data,
        "language": "en"
    }),
    options: Default::default(),
};

let transcription = ai_system.infer(speech_recognition).await?;
println!("Transcription: {:?}", transcription.output);

// æƒ…æ„Ÿåˆ†æ
let emotion_detection = AIRequest {
    model: "emotion-classifier".to_string(),
    task: AITask::EmotionRecognition,
    input: serde_json::json!({
        "audio": audio_data,
        "sample_rate": 16000
    }),
    options: Default::default(),
};

let emotions = ai_system.infer(emotion_detection).await?;
println!("Emotions: {:?}", emotions.output);
```

### è·¨æ¨¡æ€èåˆ

```rust
// è§†è§‰é—®ç­”
let vqa_request = AIRequest {
    model: "blip-vqa".to_string(),
    task: AITask::VisualQuestionAnswering,
    input: serde_json::json!({
        "image": image_data,
        "question": "What color is the car in the image?"
    }),
    options: Default::default(),
};

let answer = ai_system.infer(vqa_request).await?;
println!("Answer: {:?}", answer.output);

// å›¾åƒæè¿°ç”Ÿæˆ
let caption_request = AIRequest {
    model: "blip-caption".to_string(),
    task: AITask::ImageCaptioning,
    input: serde_json::json!({
        "image": image_data,
        "max_length": 50
    }),
    options: Default::default(),
};

let caption = ai_system.infer(caption_request).await?;
println!("Caption: {:?}", caption.output);
```

## ğŸ”— Sira AI Gatewayé›†æˆ

### ç½‘å…³é…ç½®

```rust
let sira_config = SiraConfig {
    gateway_url: "http://sira-gateway:8080".to_string(),
    api_key: "your-sira-api-key".to_string(),
    region: "us-west-2".to_string(),
    enable_load_balancing: true,
    enable_failover: true,
    connection_pool_size: 20,
    request_timeout: Duration::from_secs(30),
};

let sira_client = SiraClient::new(sira_config).await?;
```

### æ™ºèƒ½è·¯ç”±

```rust
// è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜AIæä¾›å•†
let intelligent_request = AIRequest {
    model: "auto".to_string(), // è‡ªåŠ¨é€‰æ‹©
    task: AITask::TextGeneration,
    input: serde_json::json!({
        "prompt": "Explain quantum computing",
        "max_tokens": 1000
    }),
    options: AIRequestOptions {
        use_sira_routing: true,
        routing_criteria: vec![
            RoutingCriterion::Cost,
            RoutingCriterion::Performance,
            RoutingCriterion::Availability,
        ],
        ..Default::default()
    },
};

let result = ai_system.infer_with_sira(intelligent_request).await?;
println!("Best provider result: {:?}", result.output);
```

### è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»

```rust
// å¤šæä¾›å•†è´Ÿè½½å‡è¡¡
let load_balanced_request = AIRequest {
    model: "gpt-4".to_string(),
    task: AITask::TextGeneration,
    input: serde_json::json!({"prompt": "Hello world"}),
    options: AIRequestOptions {
        enable_load_balancing: true,
        providers: vec![
            "openai".to_string(),
            "anthropic".to_string(),
            "google".to_string(),
        ],
        failover_enabled: true,
        max_retries: 3,
        ..Default::default()
    },
};

let result = ai_system.infer_load_balanced(load_balanced_request).await?;
println!("Load balanced result: {:?}", result.output);
```

## ğŸ’¾ æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

### ç»“æœç¼“å­˜

```rust
// è‡ªåŠ¨ç¼“å­˜æ¨ç†ç»“æœ
let cache_config = CacheConfig {
    enabled: true,
    ttl: Duration::from_secs(3600), // 1å°æ—¶
    max_size: 1000, // æœ€å¤šç¼“å­˜1000ä¸ªç»“æœ
    eviction_policy: EvictionPolicy::LRU,
};

let cached_request = AIRequest {
    model: "text-embedding-ada-002".to_string(),
    task: AITask::TextEmbedding,
    input: serde_json::json!({"text": "Hello world"}),
    options: AIRequestOptions {
        caching_enabled: true,
        cache_key: Some("hello_world_embedding".to_string()),
        ..Default::default()
    },
};

// é¦–æ¬¡æ¨ç† - è®¡ç®—ç»“æœå¹¶ç¼“å­˜
let first_result = ai_system.infer(cached_request.clone()).await?;
println!("First inference took: {:?}", first_result.duration);

// ç¬¬äºŒæ¬¡æ¨ç† - ä»ç¼“å­˜è¿”å›
let cached_result = ai_system.infer(cached_request).await?;
println!("Cached result took: {:?}", cached_result.duration);
```

### æ¨¡å‹é¢„çƒ­

```rust
// é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹
ai_system.preload_model("gpt-4").await?;
ai_system.preload_model("resnet50").await?;
ai_system.preload_model("whisper-base").await?;

// æ‰¹é‡é¢„çƒ­
let models_to_warm = vec![
    "bert-base-uncased",
    "efficientnet-b0",
    "yolov5s",
];

ai_system.warm_up_models(models_to_warm).await?;
println!("Models warmed up successfully");
```

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### å®æ—¶æŒ‡æ ‡

```rust
// è·å–ç³»ç»ŸæŒ‡æ ‡
let metrics = ai_system.get_metrics().await?;
println!("Active requests: {}", metrics.active_requests);
println!("Completed requests: {}", metrics.completed_requests);
println!("Average latency: {}ms", metrics.avg_latency_ms);
println!("Cache hit rate: {:.2}%", metrics.cache_hit_rate * 100.0);
println!("Error rate: {:.2}%", metrics.error_rate * 100.0);

// æ¨¡å‹ç‰¹å®šæŒ‡æ ‡
for (model_name, model_metrics) in &metrics.model_metrics {
    println!("Model {}: {} requests, {}ms avg latency",
             model_name,
             model_metrics.request_count,
             model_metrics.avg_latency_ms);
}
```

### æ€§èƒ½åˆ†æ

```rust
// è¯¦ç»†æ€§èƒ½åˆ†æ
let analysis = ai_system.analyze_performance(
    chrono::Duration::hours(1) // è¿‡å»1å°æ—¶
).await?;

println!("Performance bottlenecks:");
for bottleneck in &analysis.bottlenecks {
    println!("  {}: {}ms average latency", bottleneck.model, bottleneck.avg_latency);
}

println!("Optimization recommendations:");
for recommendation in &analysis.recommendations {
    println!("  - {}", recommendation);
}
```

### è‡ªåŠ¨ä¼˜åŒ–

```rust
// å¯ç”¨è‡ªåŠ¨æ€§èƒ½ä¼˜åŒ–
ai_system.enable_auto_optimization(AutoOptimizationConfig {
    enabled: true,
    optimization_interval: Duration::from_secs(300), // æ¯5åˆ†é’Ÿä¼˜åŒ–ä¸€æ¬¡
    max_cache_size_adjustment: 0.2, // æœ€å¤§ç¼“å­˜å¤§å°è°ƒæ•´20%
    enable_model_unloading: true, // å¯ç”¨æœªä½¿ç”¨æ¨¡å‹å¸è½½
    performance_target: PerformanceTarget {
        max_latency_ms: 500,
        min_cache_hit_rate: 0.8,
        max_error_rate: 0.01,
    },
}).await?;

println!("Auto-optimization enabled");
```

## ğŸ”§ æ’ä»¶ç³»ç»Ÿ

### æ¨¡å‹æ’ä»¶å¼€å‘

```rust
#[async_trait]
pub trait AIModelPlugin: Send + Sync {
    fn name(&self) -> &str;
    fn supported_tasks(&self) -> Vec<AITask>;
    fn model_info(&self) -> ModelInfo;

    async fn load_model(&self, model_path: &str) -> Result<ModelHandle>;
    async fn infer(&self, model: &ModelHandle, input: serde_json::Value) -> Result<serde_json::Value>;
    async fn unload_model(&self, model: &ModelHandle) -> Result<()>;
}

// è‡ªå®šä¹‰æ¨¡å‹æ’ä»¶å®ç°
pub struct CustomModelPlugin {
    client: reqwest::Client,
}

#[async_trait]
impl AIModelPlugin for CustomModelPlugin {
    fn name(&self) -> &str { "custom-model" }

    fn supported_tasks(&self) -> Vec<AITask> {
        vec![AITask::TextGeneration, AITask::TextClassification]
    }

    fn model_info(&self) -> ModelInfo {
        ModelInfo {
            name: "custom-model".to_string(),
            version: "1.0.0".to_string(),
            framework: "custom".to_string(),
            input_format: "json".to_string(),
            output_format: "json".to_string(),
            capabilities: vec!["generation".to_string(), "classification".to_string()],
        }
    }

    async fn load_model(&self, model_path: &str) -> Result<ModelHandle> {
        // å®ç°æ¨¡å‹åŠ è½½é€»è¾‘
        Ok(ModelHandle::new(model_path.to_string()))
    }

    async fn infer(&self, model: &ModelHandle, input: serde_json::Value) -> Result<serde_json::Value> {
        // è°ƒç”¨è‡ªå®šä¹‰æ¨¡å‹æ¨ç†API
        let response = self.client
            .post("http://custom-model-api:8080/infer")
            .json(&input)
            .send()
            .await?
            .json()
            .await?;

        Ok(response)
    }

    async fn unload_model(&self, model: &ModelHandle) -> Result<()> {
        // å®ç°æ¨¡å‹å¸è½½é€»è¾‘
        Ok(())
    }
}
```

### å¤„ç†å™¨æ’ä»¶

```rust
#[async_trait]
pub trait AIProcessorPlugin: Send + Sync {
    fn name(&self) -> &str;
    fn supported_modalities(&self) -> Vec<Modality>;

    async fn preprocess(&self, input: serde_json::Value) -> Result<serde_json::Value>;
    async fn postprocess(&self, output: serde_json::Value) -> Result<serde_json::Value>;
}

// å›¾åƒé¢„å¤„ç†æ’ä»¶
pub struct ImagePreprocessor;

#[async_trait]
impl AIProcessorPlugin for ImagePreprocessor {
    fn name(&self) -> &str { "image-preprocessor" }

    fn supported_modalities(&self) -> Vec<Modality> {
        vec![Modality::Vision]
    }

    async fn preprocess(&self, input: serde_json::Value) -> Result<serde_json::Value> {
        let image_data = input["image"].as_str()
            .ok_or_else(|| AIError::InvalidInput("Missing image data".to_string()))?;

        // å›¾åƒé¢„å¤„ç†é€»è¾‘
        let processed_image = process_image(image_data).await?;

        Ok(serde_json::json!({
            "processed_image": processed_image,
            "original_size": input["size"],
            "processing_time_ms": 150
        }))
    }

    async fn postprocess(&self, output: serde_json::Value) -> Result<serde_json::Value> {
        // åå¤„ç†é€»è¾‘
        let predictions = output["predictions"].as_array()
            .ok_or_else(|| AIError::InvalidOutput("Missing predictions".to_string()))?;

        let filtered_predictions: Vec<_> = predictions.iter()
            .filter(|p| p["confidence"].as_f64().unwrap_or(0.0) > 0.5)
            .cloned()
            .collect();

        Ok(serde_json::json!({
            "filtered_predictions": filtered_predictions,
            "total_predictions": predictions.len(),
            "filtered_count": filtered_predictions.len()
        }))
    }
}
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### å•å…ƒæµ‹è¯•

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_text_generation() {
        let ai_system = AISystem::new(Default::default()).await.unwrap();

        let request = AIRequest {
            model: "mock-model".to_string(),
            task: AITask::TextGeneration,
            input: serde_json::json!({"prompt": "Hello"}),
            options: Default::default(),
        };

        let response = ai_system.infer(request).await.unwrap();
        assert!(response.output.is_object());
        assert!(response.duration > Duration::from_millis(0));
    }

    #[tokio::test]
    async fn test_caching() {
        let config = AISystemConfig {
            result_cache_ttl: Duration::from_secs(300),
            ..Default::default()
        };
        let ai_system = AISystem::new(config).await.unwrap();

        let request = AIRequest {
            model: "cached-model".to_string(),
            task: AITask::TextEmbedding,
            input: serde_json::json!({"text": "test"}),
            options: AIRequestOptions {
                caching_enabled: true,
                ..Default::default()
            },
        };

        // é¦–æ¬¡æ¨ç†
        let first_result = ai_system.infer(request.clone()).await.unwrap();
        let first_duration = first_result.duration;

        // ç¬¬äºŒæ¬¡æ¨ç† (åº”è¯¥ä»ç¼“å­˜è¿”å›)
        let second_result = ai_system.infer(request).await.unwrap();
        let second_duration = second_result.duration;

        // ç¼“å­˜å‘½ä¸­åº”è¯¥æ›´å¿«
        assert!(second_duration < first_duration);
    }
}
```

### é›†æˆæµ‹è¯•

```rust
#[cfg(test)]
mod integration_tests {
    use super::*;
    use frys_kernel::FrysKernel;

    #[tokio::test]
    async fn test_full_ai_pipeline() {
        // å¯åŠ¨å®Œæ•´ç³»ç»Ÿ
        let kernel = FrysKernel::new(Default::default()).await.unwrap();
        kernel.load_plugin("ai-system").await.unwrap();

        let ai_system = AISystem::from_kernel(&kernel).await.unwrap();

        // æµ‹è¯•å¤šæ¨¡æ€ç®¡é“
        let text_input = serde_json::json!({"text": "A beautiful sunset over mountains"});
        let image_input = serde_json::json!({"image_url": "https://example.com/sunset.jpg"});

        // å¹¶å‘ç”Ÿæˆæ–‡æœ¬æè¿°å’Œå›¾åƒåˆ†æ
        let (text_result, image_result) = tokio::join!(
            ai_system.infer(AIRequest {
                model: "text-generator".to_string(),
                task: AITask::TextGeneration,
                input: text_input,
                options: Default::default(),
            }),
            ai_system.infer(AIRequest {
                model: "image-analyzer".to_string(),
                task: AITask::ImageAnalysis,
                input: image_input,
                options: Default::default(),
            })
        );

        assert!(text_result.is_ok());
        assert!(image_result.is_ok());

        kernel.shutdown().await.unwrap();
    }
}
```

## ğŸš€ éƒ¨ç½²å’Œæ‰©å±•

### Dockeréƒ¨ç½²

```dockerfile
FROM rust:1.70-slim AS builder

WORKDIR /app
COPY . .
RUN cargo build --release --bin frys-ai-system

FROM nvidia/cuda:11.8-runtime-ubuntu20.04

RUN apt-get update && apt-get install -y \
    libssl-dev \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /app/target/release/frys-ai-system /usr/local/bin/

EXPOSE 8080 9090
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:9090/health || exit 1

CMD ["frys-ai-system"]
```

### GPUæ”¯æŒ

```rust
// GPUé…ç½®
let gpu_config = GpuConfig {
    enabled: true,
    device_id: 0, // ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    memory_limit: 8 * 1024 * 1024 * 1024, // 8GB
    allow_growth: true,
    visible_devices: vec!["0".to_string()], // åªä½¿ç”¨GPU 0
};

let ai_system = AISystem::with_gpu_config(config, gpu_config).await?;
```

### Kuberneteséƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frys-ai-system
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: ai-system
        image: frys-ai-system:latest
        resources:
          requests:
            nvidia.com/gpu: "1"  # è¯·æ±‚1ä¸ªGPU
            memory: "4Gi"
            cpu: "2"
          limits:
            nvidia.com/gpu: "1"
            memory: "8Gi"
            cpu: "4"
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: FRYS_AI_GPU_ENABLED
          value: "true"
        ports:
        - containerPort: 8080
        - containerPort: 9090
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### åŸºå‡†æµ‹è¯•ç»“æœ

| ä»»åŠ¡ç±»å‹ | æ¨¡å‹ | å¹¶å‘æ•° | å¹³å‡å»¶è¿Ÿ | P95å»¶è¿Ÿ | ååé‡ |
|----------|------|--------|----------|---------|--------|
| æ–‡æœ¬ç”Ÿæˆ | GPT-3.5 | 10 | 850ms | 1200ms | 45 req/s |
| æ–‡æœ¬ç”Ÿæˆ | GPT-4 | 5 | 1800ms | 2500ms | 18 req/s |
| å›¾åƒåˆ†ç±» | ResNet50 | 20 | 45ms | 80ms | 380 req/s |
| ç›®æ ‡æ£€æµ‹ | YOLOv5 | 10 | 120ms | 200ms | 75 req/s |
| è¯­éŸ³è¯†åˆ« | Whisper | 5 | 3200ms | 4500ms | 12 req/s |

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

```rust
let optimization_config = OptimizationConfig {
    // æ¨¡å‹ä¼˜åŒ–
    enable_model_quantization: true,
    enable_onnx_runtime: true,
    enable_tensorrt: true,

    // ç¼“å­˜ä¼˜åŒ–
    result_cache_enabled: true,
    model_cache_enabled: true,
    embedding_cache_enabled: true,

    // å¹¶å‘ä¼˜åŒ–
    max_concurrent_requests: 50,
    request_queue_size: 1000,
    enable_request_batching: true,

    // èµ„æºä¼˜åŒ–
    enable_gpu_memory_pool: true,
    enable_cpu_thread_pool: true,
    memory_defragmentation_enabled: true,
};
```

## ğŸ”§ é…ç½®å’Œè°ƒä¼˜

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# åŸºç¡€é…ç½®
export FRYS_AI_MAX_CONCURRENT_REQUESTS=100
export FRYS_AI_MODEL_CACHE_SIZE=10
export FRYS_AI_RESULT_CACHE_TTL=3600

# GPUé…ç½®
export FRYS_AI_GPU_ENABLED=true
export FRYS_AI_GPU_DEVICE_ID=0
export FRYS_AI_GPU_MEMORY_LIMIT=8GB

# Siraé›†æˆé…ç½®
export FRYS_AI_SIRA_ENABLED=true
export FRYS_AI_SIRA_GATEWAY_URL=http://sira-gateway:8080
export FRYS_AI_SIRA_API_KEY=your-api-key

# ç›‘æ§é…ç½®
export FRYS_AI_MONITORING_ENABLED=true
export FRYS_AI_METRICS_INTERVAL=5
export FRYS_AI_TRACING_ENABLED=true
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### GPUå†…å­˜ä¸è¶³
```
Error: CUDA out of memory

Solution:
1. å‡å°‘å¹¶å‘è¯·æ±‚æ•°: --max-concurrent-requests 20
2. å¯ç”¨æ¨¡å‹é‡åŒ–: --enable-quantization true
3. å¢åŠ GPUå†…å­˜: ä½¿ç”¨æ›´å¤§æ˜¾å­˜çš„GPU
4. å¯ç”¨å†…å­˜æ± : --enable-gpu-memory-pool true
```

#### æ¨¡å‹åŠ è½½å¤±è´¥
```
Error: Model loading failed

Solution:
1. æ£€æŸ¥æ¨¡å‹è·¯å¾„: --model-path /path/to/models
2. éªŒè¯æ¨¡å‹æ ¼å¼: --model-format onnx
3. æ£€æŸ¥ä¾èµ–åº“: --check-dependencies true
4. å¯ç”¨æ¨¡å‹éªŒè¯: --validate-models true
```

#### Siraè¿æ¥å¤±è´¥
```
Error: Sira gateway connection failed

Solution:
1. æ£€æŸ¥ç½‘å…³URL: --sira-gateway-url http://gateway:8080
2. éªŒè¯APIå¯†é’¥: --sira-api-key valid-key
3. æ£€æŸ¥ç½‘ç»œè¿æ¥: --test-connection true
4. å¯ç”¨é‡è¯•æœºåˆ¶: --enable-retry true
```

## ğŸ“š APIå‚è€ƒ

### REST API

```http
# æ¨ç†è¯·æ±‚
POST /api/v1/infer
Content-Type: application/json

{
  "model": "gpt-4",
  "task": "text-generation",
  "input": {
    "prompt": "Hello, world!",
    "max_tokens": 100
  },
  "options": {
    "priority": "normal",
    "caching": true
  }
}

# æµå¼æ¨ç†
POST /api/v1/infer/stream
Content-Type: application/json

{
  "model": "gpt-4",
  "task": "text-generation",
  "input": {
    "prompt": "Tell me a story",
    "stream": true
  }
}

# æ‰¹é‡æ¨ç†
POST /api/v1/infer/batch
Content-Type: application/json

[
  {
    "model": "bert-base",
    "task": "text-embedding",
    "input": {"text": "First text"}
  },
  {
    "model": "bert-base",
    "task": "text-embedding",
    "input": {"text": "Second text"}
  }
]

# è·å–æŒ‡æ ‡
GET /api/v1/metrics

# å¥åº·æ£€æŸ¥
GET /api/v1/health
```

### WebSocket API

```javascript
// è¿æ¥åˆ°AIç³»ç»Ÿ
const ws = new WebSocket('ws://localhost:8080/ws/ai');

// è®¢é˜…æ¨ç†äº‹ä»¶
ws.send(JSON.stringify({
  type: 'subscribe',
  pattern: 'inference.*'
}));

// å‘é€æ¨ç†è¯·æ±‚
ws.send(JSON.stringify({
  type: 'infer',
  request_id: 'req-123',
  model: 'gpt-4',
  task: 'text-generation',
  input: { prompt: 'Hello!' }
}));

// æ¥æ”¶ç»“æœ
ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.type === 'inference_result') {
    console.log('Result:', data.result);
  }
};
```

## ğŸ¤ è´¡çŒ®

### å¼€å‘æŒ‡å—
1. Fork æœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature/new-ai-model`
3. ç¼–å†™ä»£ç å’Œæµ‹è¯•
4. è¿è¡Œæµ‹è¯•: `cargo test`
5. æäº¤PR

### æ’ä»¶å¼€å‘
1. å®ç° `AIModelPlugin` trait
2. æ·»åŠ æ’ä»¶é…ç½®
3. ç¼–å†™æ’ä»¶æ–‡æ¡£
4. æäº¤åˆ°æ’ä»¶ä»“åº“

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](../../LICENSE) æ–‡ä»¶